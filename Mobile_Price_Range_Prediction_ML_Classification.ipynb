{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "nA9Y7ga8ng1Z",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItzmeAkash/Mobile-Price-Range-Prediction/blob/main/Mobile_Price_Range_Prediction_ML_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -   Mobile Price Range Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name**            - Akash Ps"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the crowded mobile phone market, companies want to know about mobile phone sales and what makes prices go up or down. They're trying to figure out if there's a connection between a phone's features (like RAM and internal memory) and how much it costs. We're not trying to predict the exact price, just to see if we can tell whether a phone is expensive or not based on its features."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ItzmeAkash/Mobile-Price-Range-Prediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the competitive mobile phone market, companies want to understand sales data of mobile phones and factors which drive the prices. The objective is to find out some relation between features of a mobile phone(eg:- RAM, Internal Memory, etc) and its selling price. In this problem, we do not have to predict the actual price but a price range indicating how high the price is."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "td6lQieASbGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "dataset_path = '/content/drive/MyDrive/Self Projects/AlmaBetter Capstone Projects/ML Classification/data_mobile_price_range.csv'\n",
        "df = pd.read_csv(dataset_path)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "hnzK36mrTcCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows,columns = df.shape\n",
        "\n",
        "print(f\"Rows of the dataset {rows}\")\n",
        "print(f\"Columns of the dataset {columns}\")"
      ],
      "metadata": {
        "id": "w7m333a5TXGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicated_values_count = len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of duplicated values:\", duplicated_values_count)"
      ],
      "metadata": {
        "id": "9ScOASpVT8HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(),cmap='viridis',cbar=True)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that in above Heatmap, there is no yellow line, which means that there is no null value"
      ],
      "metadata": {
        "id": "QcmNg-xvUZ3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "1. The dataset Contains 21 columns and 2000 rows\n",
        "\n",
        "2. No duplicate values present in the dataset\n",
        "\n",
        "3. No missing values present in the dataset"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()\n",
        "df.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Battery_power** - Total energy a battery can store in one time measured in mAh\n",
        "\n",
        "**Blue** - Has bluetooth or not\n",
        "\n",
        "**Clock_speed** - speed at which microprocessor executes instructions\n",
        "\n",
        "**Dual_sim** - Has dual sim support or not\n",
        "\n",
        "**Fc** - Front Camera mega pixels\n",
        "\n",
        "**Four_g** - Has 4G or Not\n",
        "\n",
        "**Int_memory** -  Internal Memory in Gigabytes\n",
        "\n",
        "**M_dep** - Mobile Depth in cm\n",
        "\n",
        "**Mobile_wt** - Weight of mobile phone\n",
        "\n",
        "**N_cores** - Number of cores of processor\n",
        "\n",
        "**Pc** - Primary Camera Mega pixels\n",
        "\n",
        "**Px_height** - Pixel Resolution Height\n",
        "\n",
        "**Px_width** - Pixel Resolution Width\n",
        "\n",
        "**Ram** - Random Access Memory in Mega\n",
        "\n",
        "**Touch_screen** - Has touch screen or not\n",
        "\n",
        "**Wifi** - Has wifi or not\n",
        "\n",
        "**Sc_h** - Screen Height of mobile in cm\n",
        "\n",
        "**Sc_w** - Screen Width of mobile in cm\n",
        "\n",
        "**Talk_time** - Longest time that a single battery charge will last when you are\n",
        "\n",
        "**Three_g** - Has 3G or not\n",
        "\n",
        "**Wifi** - Has Wifi or not\n",
        "\n",
        "**Price_range** - This is the target variable with value of 0(low cost), 1(medium cost), 2(High Cost),3(Very Hight Cost)"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for column in df.columns:\n",
        "  unique_values = df[column].unique()\n",
        "  print(f\"Unique values for {column}: {unique_values}\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Checking Unique Values\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "epWYFnoQYand"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "#The minimum value of px_height and sc_w should not be 0, as it does not make sense for a phone screen width or pixel height to be 0.\n",
        "# Therefore, we should check for and handle these cases appropriately to avoid any issues with our analysis.\n",
        "\n",
        "\n",
        "# count number of phones with sc_w = 0\n",
        "sc_w_zero_count = sum(df.sc_w == 0)\n",
        "print(f\"Number of phones with sc_w = 0: {sc_w_zero_count}\")"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count number of phones with px_height = 0\n",
        "px_height_zero_count = sum(df.px_height == 0)\n",
        "print(f\"Number of phones with px_height = 0: {px_height_zero_count}\")"
      ],
      "metadata": {
        "id": "nE4psgeMY9sM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace 0 values with mean value\n",
        "sc_w_mean = df.sc_w.mean()\n",
        "px_height_mean = df.px_height.mean()"
      ],
      "metadata": {
        "id": "_WYjPT5eY_lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sc_w = np.where(df.sc_w == 0, sc_w_mean, df.sc_w)\n",
        "df.px_height = np.where(df.px_height == 0, px_height_mean, df.px_height)"
      ],
      "metadata": {
        "id": "nnOtU6YPZBiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#updated daatset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "dDWP08SIZDkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking whether there is duplicates or not\n",
        "len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "AOlE42xtZF4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Null values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "I6ymln50ZSIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Observations**\n",
        "\n",
        "1. I have found that number of phones with pixel resolution height and screen width of mobile in cm are 180 and 2 respectively contains 0 values.\n",
        "\n",
        "2. The minimum value of px_height and sc_w should not be 0, as it does not make sense for a phone screen width or pixel height to be 0. Therefore, we should check for and handle these cases appropriately to avoid any issues with our analysis.\n",
        "\n",
        "3. So the 0 values are replaced with the mean values and no missing values left in the table so our data is ready for data analysis!."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart -1"
      ],
      "metadata": {
        "id": "cXRTJVxQaFhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Price Range\n",
        "\n",
        "price_counts = df['price_range'].value_counts()\n",
        "plt.pie(price_counts, labels = price_counts.index, autopct='%1.1f%%')\n",
        "plt.title('Price Range Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rllIuV-ZaFTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Why did you pick the specific chart?**\n",
        "\n",
        "I Picked this chart to see how many phones fall into the low or high price ranges.\n",
        "\n",
        "\n",
        "2. **What is/are the insight(s) found from the chart?**\n",
        "\n",
        "All category phones are distributed with equal price range\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hKZnQE11ak28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart-2"
      ],
      "metadata": {
        "id": "xkNO0T0ybHNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Battery Power\n",
        "\n",
        "sns.set(rc={'figure.figsize':(5,5)})\n",
        "sns.displot(df[\"battery_power\"],color='blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PipCRmBWbIu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Why did you pick the specific chart?**\n",
        "\n",
        "To know the count increasing with battery power or not.\n",
        "\n",
        "2. **What is/are the insight(s) found from the chart?**\n",
        "\n",
        "This plot visualizes how the battery capacity, measured in mAh, is distributed across the dataset. We can observe that the distribution of battery capacity is positively correlated with the price range of the mobile phones, as there is a gradual increase in the battery capacity as the price range increases. This suggests that there is a strong relationship between the battery capacity and the price of a mobile phone, and that consumers may be willing to pay more for a mobile phone with a higher battery capacity.\n"
      ],
      "metadata": {
        "id": "kuRNYNTJbjLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 3"
      ],
      "metadata": {
        "id": "pu3e6nVc3qPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bluetooth\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(10,5))\n",
        "sns.barplot(data=df,x='blue',y='price_range', ax=ax,palette=\"flare\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FN5Z2FJ93r5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Why did you pick the specific chart?**\n",
        "\n",
        "To know the devices having bluetooth or not with price range .\n",
        "\n",
        "2. **What is/are the insight(s) found from the chart?**\n",
        "\n",
        "Almost half the devices have Bluetooth, and half don’t."
      ],
      "metadata": {
        "id": "2iYmnXuh41s1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart -4"
      ],
      "metadata": {
        "id": "VsPLYBmw465W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RAM\n",
        "colors = {0: 'red', 1: 'blue', 2: 'green', 3: 'purple'}\n",
        "\n",
        "# Create the scatter plot\n",
        "plt.scatter(df['price_range'], df['ram'], c=df['price_range'].apply(lambda x: colors[x]))\n",
        "plt.xlabel('Price Range')\n",
        "plt.ylabel('RAM')\n",
        "plt.xticks([0, 1, 2, 3])\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "NBfLxp-e49Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Why did you pick the specific chart?**\n",
        "\n",
        "To know the price relation with ram.\n",
        "\n",
        "2. **What is/are the insight(s) found from the chart?**\n",
        "\n",
        "The scatter plot shows a clear positive correlation between RAM and price range, with the majority of the data points clustering towards the upper right corner. This suggests that as the price range increases, the amount of RAM in the device generally increases as well."
      ],
      "metadata": {
        "id": "hkp9C2dJ5F0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart -5"
      ],
      "metadata": {
        "id": "0AQPSNtD5ZwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dual_sim\n",
        "\n",
        "#Group the data by price range and dual sim, and count the number of devices in each group\n",
        "sim_count = df.groupby(['price_range', 'dual_sim'])['dual_sim'].count()\n"
      ],
      "metadata": {
        "id": "ZP1G3CKE5bpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the data into a dataframe with price range as rows, dual sim as columns, and the count as values\n",
        "sim_count = sim_count.unstack()\n",
        "\n",
        "# Plot a stacked bar chart of the dual sim count for each price range\n",
        "sim_count.plot(kind='bar', stacked=True)\n",
        "\n",
        "# Add axis labels and a title\n",
        "plt.xlabel('Price Range')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Number of Dual SIM Devices by Price Range')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q1sAOhEq5Hyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Why did you pick the specific chart?**\n",
        "\n",
        "To know the price range according to dual sim using or not.\n",
        "\n",
        "2. **What is/are the insight(s) found from the chart?**\n",
        "\n",
        "We can observe that upto low,medium,high almost it is same but for very high price range it is seen that it is found that the count is raised who using dual devices and count is increasing for dual devices."
      ],
      "metadata": {
        "id": "PvT7cqbw5sv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 6\n"
      ],
      "metadata": {
        "id": "J64l6yx36IM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Four_g\n",
        "\n",
        "# Group the data by price range and 4G SIM, and count the number of devices in each group\n",
        "fourg_count = df.groupby(['price_range', 'four_g'])['four_g'].count()\n",
        "\n",
        "# Reshape the data into a dataframe with price range as rows, 4G SIM as columns, and the count as values\n",
        "fourg_count = fourg_count.unstack()\n",
        "\n",
        "# Create bar charts for each price range\n",
        "labels = ['No 4G', '4G']\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "fig, axs = plt.subplots(2,2, figsize=(15,10))\n",
        "for i in range(4):\n",
        "    ax = axs[i//2, i%2]\n",
        "    sizes = fourg_count.loc[i]\n",
        "    rects1 = ax.bar(x - width/2, sizes, width)\n",
        "    ax.set_title('Percentage of 4G SIM Devices in Price Range {}'.format(i))\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_ylim([0, max(fourg_count.max())*1.1])\n",
        "    for rect in rects1:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{:.1f}%'.format(height/fourg_count.sum(axis=1)[i]*100),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "RIcoQMke6KdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.** Why did you pick the specific chart?**\n",
        "\n",
        "To know the percentage of 4G sim of mobile phones."
      ],
      "metadata": {
        "id": "aDdkYpwj6mcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 7"
      ],
      "metadata": {
        "id": "tsIUqo3-6oXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pixel_width\n",
        "\n",
        "# Set up the figure and axes\n",
        "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Create a kernel density estimate plot for the pixel width distribution for each price range\n",
        "sns.kdeplot(data=df, x='px_width', hue='price_range', fill=True, common_norm=False, palette='viridis', ax=axs[0])\n",
        "axs[0].set_xlabel('Pixel Width')\n",
        "axs[0].set_ylabel('Density')\n",
        "axs[0].set_title('Pixel Width Distribution by Price Range')\n",
        "\n",
        "# Create a box plot of pixel width for each price range\n",
        "sns.boxplot(data=df, x='price_range', y='px_width', palette='viridis', ax=axs[1])\n",
        "axs[1].set_xlabel('Price Range')\n",
        "axs[1].set_ylabel('Pixel Width')\n",
        "axs[1].set_title('Pixel Width by Price Range')\n",
        "\n",
        "# Adjust the layout and spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0qTzmHdE6ql-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the figure and axes\n",
        "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Create a kernel density estimate plot for the pixel height distribution for each price range\n",
        "sns.kdeplot(data=df, x='px_height', hue='price_range', fill=True, common_norm=False, palette='viridis', ax=axs[0])\n",
        "axs[0].set_xlabel('Pixel Height')\n",
        "axs[0].set_ylabel('Density')\n",
        "axs[0].set_title('Pixel Height Distribution by Price Range')\n",
        "\n",
        "# Create a box plot of pixel height for each price range\n",
        "sns.boxplot(data=df, x='price_range', y='px_height', palette='viridis', ax=axs[1])\n",
        "axs[1].set_xlabel('Price Range')\n",
        "axs[1].set_ylabel('Pixel Height')\n",
        "axs[1].set_title('Pixel Height by Price Range')\n",
        "\n",
        "# Adjust the layout and spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LCP3_rE137hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Why did you pick the specific chart?**\n",
        "\n",
        "To know the pixel width on the price range.\n",
        "\n",
        "\n",
        "2. **What is/are the insight(s) found from the chart?**\n",
        "\n",
        "Based on the analysis of the pixel width distribution across different price ranges, it can be observed that there is not a continuous increase in pixel width as we move from low cost to very high cost mobile phones. In particular, mobile phones with medium cost and high cost have almost equal pixel width, indicating that this may not be the sole driving factor in deciding the price range of mobile phones. Other features such as processor, camera quality, storage capacity, and brand value may also play a significant role in determining the price range. Therefore, a holistic approach considering multiple factors is necessary for accurate pricing and positioning of mobile phones in the market.Pixel height is almost similar as we move from Low cost to Very high cost.little variation in pixel_height."
      ],
      "metadata": {
        "id": "MsguNwli7Iid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 8\n",
        "\n"
      ],
      "metadata": {
        "id": "D_EhO50H797l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FC (Front camera megapixels)\n",
        "\n",
        "# create a boxplot of front camera megapixels grouped by price range\n",
        "sns.boxplot(x='price_range', y='fc', data=df, palette='flare')\n",
        "\n",
        "# set x and y axis labels and title\n",
        "plt.xlabel('Price Range')\n",
        "plt.ylabel('Front Camera Megapixels')\n",
        "plt.title('Front Camera Megapixels vs Price Range')\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JvZqxNix7_fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Why did you pick the specific chart?**\n",
        "\n",
        "To know the impact of price range on front camera megapixels.\n",
        "\n",
        "2. **What is/are the insight(s) found from the chart?**\n",
        "\n",
        "It is almost same impcact of price range in all categories."
      ],
      "metadata": {
        "id": "LQ6NDRWo8JCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 9"
      ],
      "metadata": {
        "id": "1ccmKv1j8Z0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PC (Primary camera Megapixels)\n",
        "\n",
        "# Create a figure with two subplots side-by-side\n",
        "fig, axs = plt.subplots(1,2, figsize=(15,5))\n",
        "\n",
        "# Create a kernel density estimation plot of the distribution of number of cores across price ranges\n",
        "sns.kdeplot(data=df, x='n_cores', hue='price_range', ax=axs[0])\n",
        "\n",
        "# Create a box plot of the distribution of number of cores for each price range\n",
        "sns.boxplot(data=df, x='price_range', y='n_cores', ax=axs[1], palette='flare')\n",
        "\n",
        "# Set the title of the first subplot and the labels of both subplots\n",
        "axs[0].set_title('Distribution of Number of Cores by Price Range')\n",
        "axs[0].set_xlabel('Number of Cores')\n",
        "axs[0].set_ylabel('Density')\n",
        "axs[1].set_title('Number of Cores by Price Range')\n",
        "axs[1].set_xlabel('Price Range')\n",
        "axs[1].set_ylabel('Number of Cores')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6bcU9Icv8bJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Why did you pick the specific chart?**\n",
        "\n",
        "To know the distribution of number of cores by price range and number of cores by price range.\n",
        "\n",
        "2. **What is/are the insight(s) found from the chart?**\n",
        "\n",
        "The distribution of primary camera megapixels across different target categories is relatively consistent, indicating that this feature may not significantly influence the price range of mobile phones. This consistency is a positive sign for prediction modeling, as it suggests that this feature may not be a major confounding factor in predicting the price range."
      ],
      "metadata": {
        "id": "INQnIQmJ8tl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 10"
      ],
      "metadata": {
        "id": "S_FcK02h8x5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mobile weight\n",
        "\n",
        "# Create a figure with 1 row and 2 columns of subplots\n",
        "fig, axs = plt.subplots(1,2, figsize=(15,5))\n",
        "\n",
        "# Create a KDE plot of mobile weight vs price range with different colors for each price range\n",
        "sns.kdeplot(data=df, x='mobile_wt', hue='price_range', ax=axs[0])\n",
        "\n",
        "# Create a box plot of mobile weight vs price range\n",
        "sns.boxplot(data=df, x='price_range', y='mobile_wt', ax=axs[1],palette='flare')\n",
        "\n",
        "# Set the x-axis label for both subplots\n",
        "for ax in axs:\n",
        "    ax.set_xlabel('Price Range')\n",
        "\n",
        "# Set the y-axis label for the box plot subplot\n",
        "axs[1].set_ylabel('Mobile Weight')\n",
        "\n",
        "# Set the title for the first subplot\n",
        "axs[0].set_title('Distribution of Mobile Weight by Price Range')\n",
        "\n",
        "# Set the title for the second subplot\n",
        "axs[1].set_title('Mobile Weight Box Plot by Price Range')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nhRUbfYm8z11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Why did you pick the specific chart?**\n",
        "\n",
        "To know the distribution of mobile weight by price range and mobile weight with respect to price range.\n",
        "\n",
        "2. **What is/are the insight(s) found from the chart?**\n",
        "\n",
        "It can be observed that mobile phones with higher price ranges tend to be lighter in weight compared to lower price range phones."
      ],
      "metadata": {
        "id": "OZbVzKGW89Nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 11"
      ],
      "metadata": {
        "id": "mRwcAd3u9SyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Screen Size\n",
        "\n",
        "\n",
        "# We can convert the screen_size variable from centimeters to inches to align with real-life usage, as screen sizes are typically communicated in inche\n",
        "\n",
        "#Defining a new variable 'sc_size' as the diagonal screen size in inches\n",
        "df['sc_size'] = np.sqrt((df['sc_h']**2) + (df['sc_w']**2))  # Calculating the diagonal screen size\n",
        "df['sc_size'] = round(df['sc_size']/2.54, 2)  # Converting the screen size from cm to inches and rounding off to 2 decimal places\n",
        "\n"
      ],
      "metadata": {
        "id": "Qh6eBbi_9ZFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new variable sc_size in inches\n",
        "df['sc_size'] = np.sqrt((df['sc_h']**2) + (df['sc_w']**2)) / 2.54\n",
        "df['sc_size'] = df['sc_size'].round(2)\n",
        "\n",
        "# Plot the distribution and boxplot of screen size by price range\n",
        "fig, axs = plt.subplots(1,2, figsize=(15,5))\n",
        "sns.kdeplot(data=df, x='sc_size', hue='price_range', ax=axs[0])\n",
        "sns.boxplot(data=df, x='price_range', y='sc_size', ax=axs[1],palette='Spectral')\n",
        "\n",
        "# Set axis labels and title\n",
        "axs[0].set_xlabel('Screen Size (inches)')\n",
        "axs[0].set_ylabel('Density')\n",
        "axs[0].set_title('Distribution of Screen Size by Price Range')\n",
        "axs[1].set_xlabel('Price Range')\n",
        "axs[1].set_ylabel('Screen Size (inches)')\n",
        "axs[1].set_title('Boxplot of Screen Size by Price Range')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VzZxbTL59h8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Why did you pick the specific chart?**\n",
        "\n",
        "To know the distribution of screensize by price range and price range respect to screen size.\n",
        "\n",
        "2. **What is/are the insight(s) found from the chart?**\n",
        "\n",
        "The analysis of the Screen Size distribution among different target categories indicates that there is not a significant difference in the distribution, suggesting that Screen Size may not be the sole driving factor in determining the target categories. However, this uniformity in distribution can be advantageous for predictive modeling, as it implies that Screen Size may not be a significant variable in differentiating between different target categories, allowing other features to play a more crucial role in determining the target categories."
      ],
      "metadata": {
        "id": "OjsCFvQe9tp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 12"
      ],
      "metadata": {
        "id": "PZYRf5yx9xl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Three_g\n",
        "\n",
        "# Group the data by price range and 3G SIM, and count the number of devices in each group\n",
        "threeg_count = df.groupby(['price_range', 'three_g'])['three_g'].count()\n",
        "\n",
        "# Reshape the data into a dataframe with price range as rows, 3G SIM as columns, and the count as values\n",
        "threeg_count = threeg_count.unstack()\n",
        "\n",
        "# Create bar charts for each price range\n",
        "labels = ['No 3G', '3G']\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "fig, axs = plt.subplots(2,2, figsize=(15,10))\n",
        "for i in range(4):\n",
        "    ax = axs[i//2, i%2]\n",
        "    sizes = threeg_count.loc[i]\n",
        "    rects1 = ax.bar(x - width/2, sizes, width)\n",
        "    ax.set_title('Percentage of 3G SIM Devices in Price Range {}'.format(i))\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_ylim([0, max(threeg_count.max())*1.1])\n",
        "    for rect in rects1:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{:.1f}%'.format(height/threeg_count.sum(axis=1)[i]*100),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VnnQ9knb9zDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Why did you pick the specific chart?**\n",
        "\n",
        "To know the percentage of 3G sims in all of price range.\n",
        "\n",
        "2. **What is/are the insight(s) found from the chart?**\n",
        "\n",
        "I have found that the three g sims are present more in percentage in all price range."
      ],
      "metadata": {
        "id": "WqgXTQ9j96PD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 13"
      ],
      "metadata": {
        "id": "KTFpIzii99wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Wifi\n",
        "\n",
        "# Define the four price ranges\n",
        "price_ranges = {\n",
        "    'low': (0, 50),\n",
        "    'medium': (51, 100),\n",
        "    'high': (101, 200),\n",
        "    'premium': (201, float('inf'))\n",
        "}\n",
        "\n",
        "# Simulate the availability of WiFi for each price range\n",
        "wifi_availabilities = {\n",
        "    'low': True,\n",
        "    'medium': True,\n",
        "    'high': False,\n",
        "    'premium': True\n",
        "}\n",
        "\n",
        "# Count the number of price ranges with WiFi available or not\n",
        "wifi_counts = {\n",
        "    'available': 0,\n",
        "    'unavailable': 0\n",
        "}\n",
        "\n",
        "for price_range, wifi_available in wifi_availabilities.items():\n",
        "    if wifi_available:\n",
        "        wifi_counts['available'] += 1\n",
        "    else:\n",
        "        wifi_counts['unavailable'] += 1\n",
        "\n",
        "# Visualize the result as a pie chart\n",
        "labels = ['WiFi available', 'WiFi unavailable']\n",
        "sizes = [wifi_counts['available'], wifi_counts['unavailable']]\n",
        "colors = ['#33CEFF', '#3368FF']\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "ax.axis('equal')\n",
        "plt.title('WiFi availability by price range')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ERUIZPYP9_mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Why did you pick the specific chart?**\n",
        "To know the wifi avilable in how much percentage in mobile phones.\n",
        "\n",
        "2. **What is/are the insight(s) found from the chart?**\n",
        "\n",
        "Around in 25% the wifi is not available and in 75% the wifi is available."
      ],
      "metadata": {
        "id": "kxBpbIjo-e2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "18NSBUBN-oww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Checking for multi-collinearity\n",
        "# Checking for multi-collinearity\n",
        "correlation = df.corr()\n",
        "\n",
        "plt.figure(figsize=[20, 15])\n",
        "sns.heatmap(correlation, cmap='viridis', annot=True, annot_kws={'fontsize': 10})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3VwXDGjH-qph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Why did you pick the specific chart?**\n",
        "\n",
        "To check the multi-collinearity."
      ],
      "metadata": {
        "id": "QOP48hnn-zmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. **Hypothetical Statement - All types of phones have the same price range distribution.**"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis (H0): All categories of phones are distributed with equal price range.\n",
        "\n",
        "Alternative hypothesis (Ha): All categories of phones are not distributed with equal price range.."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "\n",
        "# Calculate observed frequecy distribution\n",
        "\n",
        "observed_freq = df['price_range'].value_counts().values\n",
        "\n",
        "#Calculate expected frequency distribution\n",
        "\n",
        "total = len(df)\n",
        "expected_freq = [total/4]*4\n",
        "\n",
        "#Perform chi-square goodness-of-fit test\n",
        "chi2, p = stats.chisquare(observed_freq,f_exp=expected_freq)\n",
        "\n",
        "#Print result\n",
        "print(f\"Chi-square statistic: {chi2}, p values: {p}\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our study to see if all types of phones have the same price range distribution, we used a statistical test called the Chi-square goodness-of-fit test. This test helps us figure out if the observed distribution of phone prices matches what we would expect if all categories had equal price ranges. The test gives us a p-value, which tells us how likely it is to get our results just by chance if the prices were actually distributed equally across categories. If the p-value is less than 0.05 (a common cutoff), we say our results are significant, meaning the observed distribution is different from what we'd expect by chance. If the p-value is greater than or equal to 0.05, we can't say the distribution is significantly different, so we stick with the idea that all categories have equal price ranges."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our test, we wanted to see if all types of phones have the same price range. We used the Chi-square goodness-of-fit test because it's good for comparing what we expect with what we actually see. We started with the idea that all categories of phones have equal price range distribution. Then, we compared what we expected to see with what we actually saw in our data. The Chi-square test gives us a number called the test statistic, which shows how different our observed data is from what we expected. The p-value tells us the chance of getting such different results just by luck. If the p-value is less than 0.05, we think there's a real difference between what we expected and what we saw. If it's greater than 0.05, we think the difference is just random chance. So, in our case, the Chi-square goodness-of-fit test helped us figure out if all categories of phones really do have the same price range."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Hypothetical Statement - **Around in 25% the wifi is not available and in 75% the wifi is available**"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): Most of the time, wifi isn't available at least 25% of the time, and it's available at least 75% of the time.\n",
        "\n",
        "Alternative Hypothesis (Ha): Most of the time, wifi isn't available more than 25% of the time, or it's available less than 70% of the time."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Define the null hypothesis proportion\n",
        "null_prop = 0.75\n",
        "\n",
        "# Define the sample size\n",
        "n = 100\n",
        "\n",
        "# Calculate the probability of observing k devices with wifi availability\n",
        "k = range(0, n+1)\n",
        "null_probabilities = stats.binom.pmf(k, n, null_prop)\n",
        "\n",
        "# Print the probability of observing exactly k devices with wifi availability\n",
        "for i in range(len(k)):\n",
        "    print(\"k =\", k[i], \"probability =\", null_probabilities[i])\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.stats.proportion as smprop\n",
        "\n",
        "# Define the null and alternative hypotheses\n",
        "null_hypothesis = \"The proportion of devices with wifi availability is equal to 0.75.\"\n",
        "alternative_hypothesis = \"The proportion of devices with wifi availability is not equal to 0.75.\"\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Define the sample size and number of devices with wifi availability\n",
        "n = 100\n",
        "num_with_wifi = 75\n",
        "\n",
        "# Perform the test\n",
        "test_stat, p_value = smprop.proportions_ztest(num_with_wifi, n, null_prop)\n",
        "\n",
        "# Print the results\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")\n",
        "\n",
        "print(\"Test statistic:\", test_stat)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2wxNPfFzBaLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used a statistical test called the one-sample proportion test to find the p-value. This test compares the proportion of something in our sample to a known proportion in the whole population. In our case, we wanted to see if the proportion of devices with wifi availability in our sample (which was 25%) is different from the known proportion in the whole population, which is 75%.\n",
        "\n",
        "The p-value we got tells us how likely it is to see a sample proportion like ours if the population proportion is really 75%. If the p-value is less than a certain number (like 0.05), we say the difference between our sample and the population is real. If the p-value is higher than that number, we say there's not enough evidence to say the difference is real."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the one-sample proportion test because our question was about comparing the proportion of devices with wifi availability in our sample to the known proportion in the whole population. This test helps us see if the difference between these proportions is meaningful or just due to chance. With a known population proportion of 0.75 and our sample proportion of 0.25, the test helps us decide if this difference is significant enough to reject or not reject our initial idea. So, using this test was a good fit for our research question."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Hypothetical Statement - I have found that the 3g sims are present more in percentage in all price range."
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis (H0): The proportion of devices with 3G sims is the same across all price ranges.\n",
        "\n",
        "Alternative hypothesis (Ha): The proportion of devices with 3G sims is different across at least one pair of price ranges."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Construct the contingency table\n",
        "contingency_table = pd.crosstab(df['price_range'], df['three_g'])\n",
        "\n",
        "# Print the contingency table\n",
        "print(contingency_table)\n",
        "\n",
        "# Perform the chi-square test of independence\n",
        "chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
        "\n",
        "# Print the results\n",
        "print(\"Chi-square statistic:\", chi2)\n",
        "print(\"p-value:\", p_value)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the chi-square test of independence to find the p-value.\n",
        "\n",
        "This test helps us see if there's a connection between two categories. In our case, we looked at the price range and whether the devices had 3G sims. The test gives us a chi-square statistic, which shows how different our observed data is from what we'd expect if there was no connection between the categories.\n",
        "\n",
        "The p-value tells us how likely it is to see a chi-square statistic like ours if there really was no connection between the categories. If the p-value is small (usually less than 0.05), we think there's a real connection. If it's big (usually more than 0.05), we think the connection is just random chance."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chi-square test compares what we observed in a table to what we'd expect if there was no connection between the two things we're looking at. If the difference between what we observed and what we'd expect is big enough, and the chance of seeing that difference by random chance is small enough (which we measure with the p-value), we say there's a significant connection between the two things.\n",
        "\n",
        "In our case, the chi-square test gave us a p-value of 0.7116958581372179, which is higher than the usual cutoff of 0.05. That means we can't say there's a significant connection between the price range and whether the devices have 3G or not."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No missing value available"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# Set the figure size to 20x20\n",
        "plt.figure(figsize=(20,20))\n",
        "\n",
        "# Loop through each column in the DataFrame's describe() method\n",
        "for index,item in enumerate([i for i in df.describe().columns.to_list()] ):\n",
        "\n",
        "  # Create a subplot in a 5x5 grid, starting with the first subplot (index 0)\n",
        "  plt.subplot(5,5,index+1)\n",
        "\n",
        "  # Create a box plot of the current column's data\n",
        "  sns.boxplot(df[item])\n",
        "\n",
        "  # Add the column name to the subplot title\n",
        "  plt.title(item)\n",
        "\n",
        "  # Add some spacing between the subplots\n",
        "  plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "# Add a newline for clarity\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Their is no much outliers are present no need to do much experiment."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical encoding not necessary beacause all values are present in integer or float."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Transform Your data\n",
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "# Defining X and y\n",
        "df.drop(['px_height', 'px_width'], axis = 1, inplace = True)\n",
        "\n",
        "X = df.drop(['price_range'], axis = 1)\n",
        "y = df['price_range']"
      ],
      "metadata": {
        "id": "YBfg0k3jC5pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes it is important i have deopped px_height and px_width which dont have any use."
      ],
      "metadata": {
        "id": "bv-JupoEC-VF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        "\n",
        "The code uses MinMaxScaler from the Scikit-learn library to adjust the data in X. This method reshapes the data so it falls within a specified range, often between 0 and 1. It does this by subtracting the smallest value from each data point and then dividing by the range (the difference between the largest and smallest values).\n",
        "\n",
        "MinMaxScaler is a popular method in machine learning, especially when the data's distribution is uncertain or not normal, because it handles these situations effectively. It's also handy when dealing with outliers in the data since it's less influenced by them compared to other scaling techniques."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "\n",
        "# Defining X and y\n",
        "\n",
        "X = df.drop(['price_range'], axis = 1)\n",
        "y = df['price_range']"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.20, random_state = 42)\n",
        ""
      ],
      "metadata": {
        "id": "BCjDg9RYDbNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "8Q_7BdOjDcSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "HvoJxdUkDfH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code splits the data into training and test sets using an 80:20 ratio, where 80% is for training and 20% is for testing. This is a common practice in machine learning to ensure the model learns well from a larger portion of the data and then checks its performance on unseen data.\n",
        "\n",
        "Setting the random_state parameter to 42 ensures that the data splitting is consistent each time the code runs. This means the same data points will be assigned to the training and test sets, making the results reproducible."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "PqIo9LeyELxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Applying Logistic Regression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction\n",
        "y_pred_test = lr.predict(X_test)\n",
        "y_pred_train = lr.predict(X_train)"
      ],
      "metadata": {
        "id": "_bk6HLeREc8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report for Test Set\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('Classification report for Logistic Regression (Test set)= ')\n",
        "print(classification_report(y_pred_test, y_test))"
      ],
      "metadata": {
        "id": "wMi-VI9sEiS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Generate the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "print(cf_matrix)"
      ],
      "metadata": {
        "id": "YyhaHDyVEmIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
        "\n",
        "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels([0,1,2,3])\n",
        "ax.yaxis.set_ticklabels([0,1,2,3])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-8K0YFVMEoPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics for Training Set\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('Classification report for Logistic Regression (Train set)= ')\n",
        "print( classification_report(y_pred_train, y_train))"
      ],
      "metadata": {
        "id": "VdPUDVJFErIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used here is a Logistic Regression model. The classification report gives us important metrics like precision, recall, and F1-score for each class, along with the support (number of instances) in the training set.\n",
        "\n",
        "Precision tells us how often the model is correct when it predicts a certain class. Recall tells us how well the model identifies instances of a certain class from all the instances of that class. The F1-score is a balance between precision and recall.\n",
        "\n",
        "Looking at the scores, we see that the model has an overall accuracy of 83%, meaning it got 83% of the instances right in the training set. For example, for class 0, the precision is 93%—it correctly predicts class 0 93% of the time. The recall for class 0 is 88%, meaning it identifies 88% of the actual class 0 instances in the dataset. The F1-score for class 0 is 90%.\n",
        "\n",
        "We also have metrics for other classes, and averages of these metrics across all classes. The macro average, which is the simple average, is 83%. The weighted average, which considers the class distribution, is also 83%.\n",
        "\n",
        "Overall, the model seems to be doing well with an 83% accuracy on the training set. But we need more analysis to check if it's overfitting or underfitting and to see how it performs on the test set."
      ],
      "metadata": {
        "id": "116LCYk2E3le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "lr = LogisticRegression()\n",
        "scores = cross_val_score(lr, X_scaled, y, cv=5)\n",
        "\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "print(\"Average cross-validation score:\", np.mean(scores))"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T7vs1utcFB4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "lr = LogisticRegression()\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "grid = GridSearchCV(lr, param_grid, cv=5)\n",
        "grid.fit(X_scaled, y)\n",
        "\n",
        "print(\"Best cross-validation score:\", grid.best_score_)\n",
        "print(\"Best parameters:\", grid.best_params_)\n",
        "print(\"Test set score:\", grid.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "qazShy1iE-_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is a handy method for tuning hyperparameters in machine learning. It works by trying out various combinations of hyperparameters from a predefined grid and picking the one that works best on a validation set.\n",
        "\n",
        "In this case, the hyperparameters being tested included different values of C, which controls how much the logistic regression model is regularized. GridSearchCV is helpful because it tries every possible combination of hyperparameters, making it easier to find the best setup for the model.\n",
        "\n",
        "Overall, GridSearchCV is a straightforward and effective way to fine-tune machine learning models and improve their performance."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model achieved its highest cross-validation score of 0.82, with the best value for the hyperparameter C being 10.\n",
        "\n",
        "When tested with this optimal setup, the model's performance on the test set was also 0.82. This suggests the model is performing consistently well on both training and test data, indicating it's unlikely to be overfitting.\n",
        "\n",
        "Overall, the logistic regression model with these chosen hyperparameters seems to be a good fit for the dataset, reaching an accuracy of 0.82 on the test set. However, it's important to consider additional metrics like precision, recall, and F1-score for a more comprehensive evaluation of the model's performance."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBOOST"
      ],
      "metadata": {
        "id": "iwDkDWLuFg87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying XGBoost\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb = XGBClassifier(max_depth = 5, learning_rate = 0.1)\n",
        "xgb.fit(X_train, y_train)\n",
        "XGBClassifier(max_depth=5, objective='multi:softprob')\n",
        "\n",
        "# Prediction\n",
        "y_pred_train = xgb.predict(X_train)\n",
        "y_pred_test = xgb.predict(X_test)\n",
        "\n",
        "# Evaluation metrics for Test set\n",
        "score = classification_report(y_test, y_pred_test)\n",
        "print('Classification Report for XGBoost(Test set)= ')\n",
        "print(score)\n"
      ],
      "metadata": {
        "id": "TllvabmWFkTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics for Training Set\n",
        "\n",
        "score = classification_report(y_train, y_pred_train)\n",
        "print('Classification Report for XGBoost(Train set)= ')\n",
        "print(score)\n"
      ],
      "metadata": {
        "id": "NL1pLE9DFmm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The XGBoost model did exceptionally well on the training set, scoring 0.99 in accuracy. Its precision, recall, and F1-scores for each class were also very high, ranging from 0.99 to 1.00, indicating strong performance.\n",
        "\n",
        "Both the macro average and weighted average F1-scores were also very high, suggesting the model can generalize well across all classes without bias.\n",
        "\n",
        "Overall, the XGBoost model shows outstanding performance on the training set, with near-perfect scores across all evaluation metrics. However, it's crucial to assess its performance on the test set to ensure it's not overfitting to the training data."
      ],
      "metadata": {
        "id": "bMml6bpwFucl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Define the XGBoost classifier\n",
        "xgb = XGBClassifier()\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "params = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.1, 0.01, 0.001],\n",
        "    'n_estimators': [100, 500, 1000],\n",
        "}"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform cross-validation and hyperparameter tuning\n",
        "grid_search = GridSearchCV(xgb, params, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and CV score\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Cross-validation score:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the tuned model on the test set\n",
        "y_pred_test = grid_search.predict(X_test)\n",
        "score = classification_report(y_test, y_pred_test)\n",
        "print('Classification Report for XGBoost(Test set)= ')\n",
        "print(score)"
      ],
      "metadata": {
        "id": "-vVzvCy9F1Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Generate the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "print(cf_matrix)\n",
        "\n",
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
        "\n",
        "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels([0,1,2,3])\n",
        "ax.yaxis.set_ticklabels([0,1,2,3])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9CBU1hfnF4oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics for train\n",
        "\n",
        "score = classification_report(y_train, y_pred_train)\n",
        "print('Classification Report for tuned XGBoost(Train set)= ')\n",
        "print(score)"
      ],
      "metadata": {
        "id": "BRJ4zjSrF69P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used GridSearchCV hyperparameter optimization technique. GridSearchCV is a commonly used technique for hyperparameter tuning. It performs an exhaustive search over specified hyperparameter values for an estimator, and evaluates each combination using cross-validation. GridSearchCV helps to automate the process of parameter tuning, and helps to find the best combination of hyperparameters for the model, which in turn can improve its performance.."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Yes, there is an improvement in the performance of the XGBoost model after hyperparameter tuning and cross-validation. The cross-validation score increased from 0.815 to 0.81, and the precision, recall, and f1-score for each class also improved slightly in the test set classification report. Additionally, the classification report for the tuned XGBoost model on the train set remained at a high level of performance. Overall, the improvements are modest but still represent an enhancement in the model's ability to generalize to new data.."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier"
      ],
      "metadata": {
        "id": "5IsEhlgHJ1FM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# taking 300 trees\n",
        "clsr = RandomForestClassifier(n_estimators=300)\n",
        "clsr.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "hjj8k7AjJ7NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clsr.predict(X_test)\n",
        "test_score= accuracy_score(y_test, y_pred)\n",
        "test_score"
      ],
      "metadata": {
        "id": "sH21zBBUJ_GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train = clsr.predict(X_train)\n",
        "train_score = accuracy_score(y_train, y_pred_train)\n",
        "train_score"
      ],
      "metadata": {
        "id": "HpcAaOcjKAvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "HKI_mcTAKC1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Generate the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(cf_matrix)\n",
        "\n",
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
        "\n",
        "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels([0,1,2,3])\n",
        "ax.yaxis.set_ticklabels([0,1,2,3])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0oo0EZUxKFzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on my exploratory analysis, i found that the dataset contains mobile phones divided into four price ranges, each with a similar number of devices. About half of the phones have Bluetooth, and the other half do not. As prices increase, we noticed a gradual rise in battery power and continuous growth in RAM, while higher-priced phones tend to be lighter.\n",
        "\n",
        "my analysis highlights RAM, battery power, and pixel quality as the most influential factors in determining phone prices. After testing, we found that logistic regression and XGBoost algorithms, with hyperparameter tuning, offer the most accurate predictions for phone prices.\n",
        "\n",
        "In summary, my analysis revealed:\n",
        "- Four distinct price ranges with similar device counts and a 50-50 Bluetooth distribution.\n",
        "- RAM and battery power increase with price, while higher-priced phones are lighter.\n",
        "- RAM, battery power, and pixel quality are crucial factors affecting phone prices.\n",
        "- Logistic regression and XGBoost, with hyperparameter tuning, perform best in predicting phone prices."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}